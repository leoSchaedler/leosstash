{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PJBL3\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wL36Zd9C1_dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando as bibliotecas necessárias"
      ],
      "metadata": {
        "id": "j2Dg7w45gLPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2, glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import datasets, layers, models, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#To ensure reproducibility\n",
        "#we set the random seed\n",
        "seed_number = 10\n",
        "tf.random.set_seed(seed_number)\n",
        "np.random.seed(seed_number)\n"
      ],
      "metadata": {
        "id": "yawwLE62Ce9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definindo função para plotar o histórico de treinamento das redes"
      ],
      "metadata": {
        "id": "xhZG3QyGgPia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot a training history\n",
        "def plot_history(history):\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "XTY29690CqkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "G-or95aZ3lSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fazendo o download do dataset pelo github"
      ],
      "metadata": {
        "id": "QSQJ_j76gWRk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxC_4uu-CUeV"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://github.com/andrehochuli/teaching/raw/main/ComputerVision/Lecture%2008%20-%20Classification/basesimpsons.zip\"\n",
        "!wget $dataset_url -O simpsons.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando diretório de treinamento e teste"
      ],
      "metadata": {
        "id": "awMSBe4VgaGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train\n",
        "!mkdir test"
      ],
      "metadata": {
        "id": "GTaCnQr6UvWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descompactando o dataset e movendo os zip para as devidas pastas"
      ],
      "metadata": {
        "id": "OoIY_6I2geen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q simpsons.zip \n",
        "!unzip -q Teste.zip -d test\n",
        "!unzip -q Treino.zip -d train"
      ],
      "metadata": {
        "id": "MoDm8oixT0QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando uma pasta para cada classe no treinamento e movendo as imagens"
      ],
      "metadata": {
        "id": "Mdm22VdbglRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train/bart\n",
        "!mkdir train/family\n",
        "!mkdir train/homer \n",
        "!mkdir train/lisa \n",
        "!mkdir train/maggie \n",
        "!mkdir train/marge\n",
        "!mv train/bart* train/bart\n",
        "!mv train/family* train/family\n",
        "!mv train/homer* train/homer \n",
        "!mv train/lisa* train/lisa \n",
        "!mv train/maggie* train/maggie \n",
        "!mv train/marge* train/marge "
      ],
      "metadata": {
        "id": "4U1v5TbEXTdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando uma pasta para cada classe no teste e movendo as imagens"
      ],
      "metadata": {
        "id": "IwXIMpzIgs5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test/bart\n",
        "!mkdir test/family\n",
        "!mkdir test/homer \n",
        "!mkdir test/lisa \n",
        "!mkdir test/maggie \n",
        "!mkdir test/marge\n",
        "!mv test/bart* test/bart\n",
        "!mv test/family* test/family\n",
        "!mv test/homer* test/homer \n",
        "!mv test/lisa* test/lisa \n",
        "!mv test/maggie* test/maggie \n",
        "!mv test/marge* test/marge "
      ],
      "metadata": {
        "id": "UF6hfDGCWmi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "definindo o diretório de dados e teste"
      ],
      "metadata": {
        "id": "YI83y6RPgypV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = '/content/train'\n",
        "test_dir = '/content/test'\n"
      ],
      "metadata": {
        "id": "BzLrXjeA6Vud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definindo um dataset de imagem para o treinamento e o teste"
      ],
      "metadata": {
        "id": "efHF9IklhOm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_ = 32\n",
        "input_shape_ = (600,400,3)\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  seed=seed_number,\n",
        "  image_size=(input_shape_[0], input_shape_[1]),\n",
        "  batch_size=batch_size_)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  test_dir,\n",
        "  seed=seed_number,  \n",
        "  image_size=(input_shape_[0], input_shape_[1]),\n",
        "  batch_size=batch_size_)"
      ],
      "metadata": {
        "id": "uFXrQZ2TswVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando a rede de 5 layers e uma layer de output\n",
        "\n",
        "-Rescaling: \n",
        "Uma camada de pré-processamento que redimensiona os valores de entrada para um novo intervalo.\n",
        "\n",
        "Essa camada redimensiona cada valor de uma entrada (geralmente uma imagem) multiplicando por escala e adicionando offset.\n",
        "\n",
        "-Conv2D: Camada de convolução 2D (por exemplo, convolução espacial sobre imagens).\n",
        "\n",
        "Essa camada cria um kernel de convolução que é convoluído com a entrada da camada para produzir um tensor de saídas.\n",
        "\n",
        "-MaxPooling: Operação de pool máximo para dados espaciais 2D.\n",
        "\n",
        "Reduz a amostra da entrada ao longo de suas dimensões espaciais (altura e largura) tomando o valor máximo em uma janela de entrada (de tamanho definido por pool_size) para cada canal da entrada. A janela é deslocada a passos largos ao longo de cada dimensão."
      ],
      "metadata": {
        "id": "Y2NyEKUWhW3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "#32 layers of size 3x3 and Relu Activation\n",
        "model.add(layers.Rescaling(1./255,input_shape=input_shape_))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "#Max Pooling of Size (2x2)\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "#64 layers of size 3x3 and Relu Activation\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "#Max Pooling of Size (2x2)\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "#64 layers of size 3x3 and Relu Activation\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "i3E3xscO4wF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully Connected:\n",
        "\n",
        "-Flatten: Achata a entrada. Não afeta o tamanho do batch.\n",
        "\n",
        "-Dense: Apenas sua camada NN densamente conectada regular.\n",
        "\n",
        "Dense implementa a operação: output = activation(dot(input, kernel) + bias) onde ativação é a função de ativação por elemento passada como argumento de ativação, kernel é uma matriz de pesos criada pela camada e bias é um vetor de viés criado pela camada (aplicável apenas se use_bias for True)."
      ],
      "metadata": {
        "id": "M-iv7FT-nJXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "N0XD32fvYvmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "8k8ddU5Kf8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_ = 20\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "EAlQeS0wIenv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "vplCEcU4AAOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred= model.predict_generator(val_ds)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "labels=val_ds.class_names\n",
        "predictions=[val_ds.class_names[k] for k in predicted_class_indices]\n",
        "print(predicted_class_indices)\n",
        "print(labels)\n",
        "print(predictions)\n",
        "y = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "print(confusion_matrix(predicted_class_indices,y))\n",
        "print(classification_report(y, predicted_class_indices, target_names=labels))"
      ],
      "metadata": {
        "id": "KMb1gtvpjZAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Augmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "MhWVlBHrcWxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O aumento de dados na análise de dados são técnicas usadas para aumentar a quantidade de dados adicionando cópias ligeiramente modificadas de dados já existentes ou dados sintéticos recém-criados a partir de dados existentes."
      ],
      "metadata": {
        "id": "3Oa25EjLqc1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(input_shape_[0],\n",
        "                                  input_shape_[1],\n",
        "                                  3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "POMIkcwoE0FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  data_augmentation, #Data Augmentation  \n",
        "  layers.Rescaling(1./255, input_shape=(96, 96, 3)),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),  \n",
        "  layers.Dropout(0.2), #Regularization\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "TfnII0ZmFyxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_ = 20\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "_DXLKr-QGRu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "lfIC_wx-M9U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred= model.predict_generator(val_ds)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "labels=val_ds.class_names\n",
        "predictions=[val_ds.class_names for k in predicted_class_indices]\n",
        "print(predicted_class_indices)\n",
        "print(labels)\n",
        "print(predictions)\n",
        "y = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "print(confusion_matrix(predicted_class_indices,y))\n",
        "print(classification_report(y, predicted_class_indices, target_names=labels))"
      ],
      "metadata": {
        "id": "IAvCcNJ4jRNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer Learning"
      ],
      "metadata": {
        "id": "bk2ufjzYoCXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O aprendizado de transferência é um problema de pesquisa em aprendizado de máquina que se concentra em armazenar o conhecimento adquirido ao resolver um problema e aplicá-lo a um problema diferente, mas relacionado. Por exemplo, o conhecimento adquirido ao aprender a reconhecer carros pode ser aplicado ao tentar reconhecer caminhões."
      ],
      "metadata": {
        "id": "Of_4c-mYqoNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layers =  tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False,\n",
        "                                                        input_tensor=tf.keras.layers.Input(input_shape_),\n",
        "                                                      classes=6)\n",
        "conv_layers.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([  \n",
        "  conv_layers,    \n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epochs_ = 10\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "SV7kpNkjSV-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "tOtHd5rFJCwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = val_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(class_names)\n",
        "class_names = np.array(class_names)\n"
      ],
      "metadata": {
        "id": "CDf9cvlHdHZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred= model.predict_generator(val_ds)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "labels=class_names\n",
        "predictions=[class_names[k] for k in predicted_class_indices]\n",
        "print(predicted_class_indices)\n",
        "print(labels)\n",
        "print(predictions)\n",
        "y = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "print(confusion_matrix(predicted_class_indices,y))\n",
        "print(classification_report(y, predicted_class_indices, target_names=labels))"
      ],
      "metadata": {
        "id": "d2sEeLzVcd7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estado da Arte"
      ],
      "metadata": {
        "id": "HRBF1uuroNg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptado de: https://www.analyticsvidhya.com/blog/2021/06/image-classification-using-convolutional-neural-network-with-python/"
      ],
      "metadata": {
        "id": "Dk76PDe8q6m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.set_random_seed(2019)"
      ],
      "metadata": {
        "id": "wg-3B15scyar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui além das camadas ja usadas, vemos camadas de dropout\n",
        "\n",
        "Dropout: Aplica Dropout à entrada.\n",
        "\n",
        "A camada Dropout define aleatoriamente as unidades de entrada para 0 com uma frequência de taxa em cada etapa durante o tempo de treinamento, o que ajuda a evitar o overfitting. As entradas não definidas como 0 são aumentadas em 1/(1 - taxa) de modo que a soma de todas as entradas não seja alterada."
      ],
      "metadata": {
        "id": "KYKaDyUNrASf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16,(3,3),activation = \"relu\" , input_shape = (180,180,3)) ,\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\") ,  \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\") ,  \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128,(3,3),activation = \"relu\"),  \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(), \n",
        "    tf.keras.layers.Dense(550,activation=\"relu\"),      #Adding the Hidden layer\n",
        "    tf.keras.layers.Dropout(0.1,seed = 2019),\n",
        "    tf.keras.layers.Dense(400,activation =\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.3,seed = 2019),\n",
        "    tf.keras.layers.Dense(300,activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.4,seed = 2019),\n",
        "    tf.keras.layers.Dense(200,activation =\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.2,seed = 2019),\n",
        "    tf.keras.layers.Dense(6,activation = \"softmax\")   #Adding the Output Layer\n",
        "])"
      ],
      "metadata": {
        "id": "HMAHN3Ce0R1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "MkUAqCH-0cBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora criamos métricas de recall, precisão e f1-score, a fim de treinar a rede para maximizar essas métricas"
      ],
      "metadata": {
        "id": "pvN998c7rYgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Er2k4NJa7Xf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora usaremos o ImageDataGenerator que gera batches de dados de imagem de tensor com data augmentation em tempo real.\n",
        "\n"
      ],
      "metadata": {
        "id": "TQ3nN7HbrkFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs=30         #Setting batch size\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "#Flow_from_directory function lets the classifier directly identify the labels from the name of the directories the image lies in\n",
        "train_generator=train_datagen.flow_from_directory(data_dir,batch_size=bs,class_mode='categorical',target_size=(180,180), shuffle=False)\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory(test_dir,\n",
        "                                                         batch_size=bs,\n",
        "                                                         class_mode  = 'categorical',\n",
        "                                                         target_size=(180,180) , shuffle=False)"
      ],
      "metadata": {
        "id": "53E2Ynlx0nU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    steps_per_epoch=150 // bs,\n",
        "                    epochs=40,\n",
        "                    validation_steps=50 // bs,\n",
        "                    verbose=2)"
      ],
      "metadata": {
        "id": "tca9PHqP1C9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "oMYmun3O2Oad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculando as métricas"
      ],
      "metadata": {
        "id": "Nqd_RThor2JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(validation_generator, verbose=0)"
      ],
      "metadata": {
        "id": "mbqlpqZK2z56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "id": "WSoBhUpG8d1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "41Nya-6p8ggi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score"
      ],
      "metadata": {
        "id": "s2ju8HIh8he5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision"
      ],
      "metadata": {
        "id": "tqwS02nY8jIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall"
      ],
      "metadata": {
        "id": "RhQ5cpBd8kMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator.reset()"
      ],
      "metadata": {
        "id": "5HC8i80VCXRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred= model.predict_generator(validation_generator)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "labels=(validation_generator.class_indices)\n",
        "labels2=dict((v,k) for k,v in labels.items())\n",
        "predictions=[labels2[k] for k in predicted_class_indices]\n",
        "print(predicted_class_indices)\n",
        "print(labels)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "4adYRzqZ90ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(predicted_class_indices)"
      ],
      "metadata": {
        "id": "fU9asciEBRs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(predicted_class_indices,validation_generator.classes))"
      ],
      "metadata": {
        "id": "nsNp9-ic_-9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(validation_generator.classes, predicted_class_indices, target_names=labels))"
      ],
      "metadata": {
        "id": "CioQ5CQeAT9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos perceber que a rede usada permite alcançar um resultado ainda maior de f1-score, o que parece alto para um banco de dados de teste desafiador com imagens que podem ser elusivas ao algoritmo."
      ],
      "metadata": {
        "id": "lrq0k3zfr6Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao comparar com a solução de extração de características pode-se dizer que a rede neural costuma performar melhor, visto que pode iterar e descobrir novos padrões no dataset até inperceptíveis para humanos, ao mesmo tempo, isso dificulta em perceber como o algoritmo funciona e como pode-se melhorar ele para identificar exatamente o que queremos.\n",
        "Na extração de característica, isso é mais óbvio.\n"
      ],
      "metadata": {
        "id": "nRTuq8G4t3gm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pteMIDMosJpo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}